{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c61623f-fc07-41bf-9d9e-28d4bb45defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AveragePrecision\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TQDMProgressBar,\n",
    ")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03357e-836b-4d45-ba3b-e6ede1f2ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROTEIN_NAMES = [\"binds_BRD4\", \"binds_HSA\", \"binds_sEH\"]\n",
    "PROTEIN_NAMES = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "data_dir = Path(\"/tokenized-chemberta\")\n",
    "model_name = \"ChemBERTa-77M-MTR\"\n",
    "batch_size = 1024 #512\n",
    "\n",
    "trainer_params = {\n",
    "  \"max_epochs\": 10,\n",
    "  \"enable_progress_bar\": True,\n",
    "  \"accelerator\": \"auto\",\n",
    "  # \"precision\": \"16-mixed\",\n",
    "  \"precision\": \"16-mixed\",\n",
    "  \"gradient_clip_val\": None,\n",
    "  \"accumulate_grad_batches\": 6,\n",
    "  \"devices\": [0,1,2,3],\n",
    "  # 'strategy': 'ddp_spawn',\n",
    "}\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174605-6061-4f20-9073-adc91a19c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        self.labels = np.array(df[PROTEIN_NAMES])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "            \"labels\": torch.tensor(self.labels[index],dtype=torch.bool),  \n",
    "        }\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e891005-ce02-483a-8a6c-7acd829c2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenize = pl.read_parquet(\n",
    "                                'hengck_train_tokenized_77-MTR_replace_dy_85M_light.parquet'\n",
    "                                #  n_rows=10000\n",
    "                                 )\n",
    "train_dataset = CustomDataset(train_tokenize)\n",
    "del train_tokenize\n",
    "\n",
    "valid_tokenize = pl.read_parquet(\n",
    "                                'hengck_valid_nonshare_tokenized_77-MTR_replace_dy.parquet',\n",
    "                                #  ,n_rows=10000\n",
    "                                 )\n",
    "valid_dataset = CustomDataset(valid_tokenize)\n",
    "del valid_tokenize\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e51cee1-a5ce-4021-98ad-67ae92378bfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class LMModel3(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(\"DeepChem/\"+model_name, num_labels=3)\n",
    "        self.lm = AutoModel.from_pretrained(\"DeepChem/\"+model_name, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        \n",
    "        self.intermediate = nn.Linear(self.config.hidden_size, 64)\n",
    "        self.intermediate_activation = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(64, self.config.num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([100.0]),reduction=\"mean\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        last_hidden_state = self.lm(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        \n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        output = sum_embeddings / sum_mask\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.intermediate(output)\n",
    "        output = self.intermediate_activation(output)\n",
    "        \n",
    "        logits = self.classifier(output)\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        output = self.forward(batch)\n",
    "        \n",
    "        loss = self.loss_fn(output[\"logits\"], batch[\"labels\"].float())\n",
    "       \n",
    "        output[\"loss\"] = loss\n",
    "        return output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffcdb6-6af2-4278-9776-4fec0fa060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBModelModule(L.LightningModule):\n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = LMModel3(model_name)\n",
    "        self.map = AveragePrecision(task=\"binary\")\n",
    "        self.map_per_class = [AveragePrecision(task=\"binary\") for _ in range(3)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "    def calculate_loss(self, batch, batch_idx):\n",
    "        return self.model.calculate_loss(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"train_loss\", ret[\"loss\"], on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return ret[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"val_loss\", ret[\"loss\"], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.map.update(F.sigmoid(ret[\"logits\"]), batch[\"labels\"].long())\n",
    "\n",
    "        for i in range(3):\n",
    "            self.map_per_class[i].update(F.sigmoid(ret[\"logits\"])[:, i], batch[\"labels\"].long()[:, i])\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_map = self.map.compute()\n",
    "        self.log(\"val_map\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        for i in range(3):\n",
    "            val_map = self.map_per_class[i].compute()\n",
    "            self.log(f\"val_map_{PROTEIN_NAMES[i]}\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "            self.map_per_class[i].reset()\n",
    "        self.map.reset()\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        logits = self.forward(batch)[\"logits\"]\n",
    "        probs = F.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,num_workers=0,collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=0, #4 pin_memory=True,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience= 2, verbose=True)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50816f5-d58e-4c83-a291-9aa0ed9bb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/\"+model_name)\n",
    "\n",
    "modelmodule = LBModelModule(model_name, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ab5950-a962-4829-9ef6-9e36aa4d283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename=f\"model_{model_name}_{{val_map:.4f}}\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_map\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"./model_chemberta\",\n",
    "    save_top_k=2,\n",
    "    verbose=1,\n",
    ")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_map\", mode=\"max\", patience=5)\n",
    "progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "    early_stop_callback,\n",
    "    progress_bar_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba27ef7a-45ac-4118-a405-ae761fc33229",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'chemberta_6_freeze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca235f59-a4a2-4945-a8f1-8f42ab4c6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(callbacks=callbacks)\n",
    "trainer.fit(modelmodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb20aa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fe0f47-d37b-48f6-a0d3-bf822987fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attention_masks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6f5db6-ef1d-4e0a-be43-6e47bf0bd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenize = pl.read_parquet(Path(data_dir, f'test_tokenized_77M_replace_dy.parquet'))\n",
    "test_dataset = CustomTestDataset(test_tokenize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True,num_workers=1,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f7ef5-2fd6-45aa-8d29-898e717e3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(callbacks=callbacks, **trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc7298-6970-413c-8434-c77650b5fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "model_path = Path(f'model_ChemBERTa-77M-MTR.ckpt')\n",
    "\n",
    "modelmodule = LBModelModule.load_from_checkpoint(\n",
    "    checkpoint_path=model_path,\n",
    "    model_name=model_name,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "predictions = trainer.predict(modelmodule, test_dataloader)\n",
    "\n",
    "predictions = torch.cat(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc633c4-e288-4430-a1a0-b01103ff5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = []\n",
    "for i, protein_name in enumerate(PROTEIN_NAMES):\n",
    "    pred_dfs.append(\n",
    "        test_tokenize.with_columns(\n",
    "            pl.lit(protein_name).alias(\"protein_name\"),\n",
    "            pl.lit(predictions[:, i]).alias(\"binds\"),\n",
    "        )\n",
    "    )\n",
    "pred_df = pl.concat(pred_dfs)\n",
    "pred_df\n",
    "submit_df = (\n",
    "    pl.read_parquet(Path(\"test.parquet\"), columns=[\"id\", \"molecule_smiles\", \"protein_name\"])\n",
    "                   \n",
    "   .join(pred_df, on=[\"id\", \"protein_name\"], how=\"left\")\n",
    "    .select([\"id\", \"binds\"])\n",
    "    .sort(\"id\")\n",
    ")\n",
    "submit_df.group_by('id').mean().write_csv(Path(f\"chemberta_dnn.csv\"))\n",
    "print(submit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca41feb-278d-4cfe-9ae6-01bd0ddcee34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
