{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c61623f-fc07-41bf-9d9e-28d4bb45defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AveragePrecision\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TQDMProgressBar,\n",
    ")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03357e-836b-4d45-ba3b-e6ede1f2ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROTEIN_NAMES = [\"binds_BRD4\", \"binds_HSA\", \"binds_sEH\"]\n",
    "PROTEIN_NAMES = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "data_dir = Path(\"/tokenized-chemberta\")\n",
    "model_name = \"ChemBERTa-77M-MTR\"\n",
    "batch_size = 1024 #512\n",
    "\n",
    "trainer_params = {\n",
    "  \"max_epochs\": 3,\n",
    "  \"enable_progress_bar\": True,\n",
    "  \"accelerator\": \"auto\",\n",
    "  # \"precision\": \"16-mixed\",\n",
    "  \"precision\": \"16-mixed\",\n",
    "  \"gradient_clip_val\": None,\n",
    "  \"accumulate_grad_batches\": 6,\n",
    "  \"devices\": [0,1,2,3],\n",
    "  # 'strategy': 'ddp_spawn',\n",
    "}\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174605-6061-4f20-9073-adc91a19c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        self.labels = np.array(df[PROTEIN_NAMES])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "            \"labels\": torch.tensor(self.labels[index],dtype=torch.bool),  \n",
    "        }\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e891005-ce02-483a-8a6c-7acd829c2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenize = pl.read_parquet(\n",
    "                                \"train_tokenized_77M-MTR_replaced_dy.parquet\"\n",
    "                                #  n_rows=10000\n",
    "                                 )\n",
    "train_dataset = CustomDataset(train_tokenize)\n",
    "del train_tokenize\n",
    "\n",
    "valid_tokenize = pl.read_parquet(\n",
    "                                'valid_tokenized_77M-MTR_replaced_dy.parquet',\n",
    "                                #  ,n_rows=10000\n",
    "                                 )\n",
    "valid_dataset = CustomDataset(valid_tokenize)\n",
    "del valid_tokenize\n",
    "gc.collect()\n",
    "\n",
    "all_data = pl.concat([train_tokenize, valid_tokenize])\n",
    "len_all_data = len(all_data)\n",
    "all_data_y = all_data[PROTEIN_NAMES].sum_horizontal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e51cee1-a5ce-4021-98ad-67ae92378bfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LMModel6(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(\"DeepChem/\"+model_name, num_labels=3)\n",
    "        self.lm = AutoModel.from_pretrained(\"DeepChem/\"+model_name, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        \n",
    "        self.intermediate1 = nn.Linear(self.config.hidden_size, 128)\n",
    "        self.intermediate_activation1 = nn.ReLU()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.intermediate2 = nn.Linear(128, 64)\n",
    "        self.intermediate_activation2 = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(64, self.config.num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        last_hidden_state = self.lm(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        output = sum_embeddings / sum_mask\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.intermediate1(output)\n",
    "        output = self.intermediate_activation1(output)\n",
    "        output = self.batch_norm1(output)\n",
    "        \n",
    "        output = self.intermediate2(output)\n",
    "        output = self.intermediate_activation2(output)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "    def calculate_loss(self, batch):\n",
    "        output = self.forward(batch)\n",
    "        loss = self.loss_fn(output[\"logits\"], batch[\"labels\"].float())\n",
    "       \n",
    "        output[\"loss\"] = loss\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffcdb6-6af2-4278-9776-4fec0fa060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBModelModule(L.LightningModule):\n",
    "    def __init__(self, model_name, batch_size):\n",
    "        super().__init__()\n",
    "        self.model = LMModel6(model_name)\n",
    "        self.map = AveragePrecision(task=\"binary\")\n",
    "        self.map_per_class = [AveragePrecision(task=\"binary\") for _ in range(3)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "    def calculate_loss(self, batch, batch_idx):\n",
    "        return self.model.calculate_loss(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"train_loss\", ret[\"loss\"], on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return ret[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ret = self.calculate_loss(batch, batch_idx)\n",
    "        self.log(\"val_loss\", ret[\"loss\"], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.map.update(F.sigmoid(ret[\"logits\"]), batch[\"labels\"].long())\n",
    "\n",
    "        for i in range(3):\n",
    "            self.map_per_class[i].update(F.sigmoid(ret[\"logits\"])[:, i], batch[\"labels\"].long()[:, i])\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_map = self.map.compute()\n",
    "        self.log(\"val_map\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        for i in range(3):\n",
    "            val_map = self.map_per_class[i].compute()\n",
    "            self.log(f\"val_map_{PROTEIN_NAMES[i]}\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "            self.map_per_class[i].reset()\n",
    "        self.map.reset()\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        logits = self.forward(batch)[\"logits\"]\n",
    "        probs = F.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True,num_workers=0,collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=0, #4 pin_memory=True,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience= 2, verbose=True)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50816f5-d58e-4c83-a291-9aa0ed9bb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/\"+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "FOLD = [0,1,2,3,4]\n",
    "\n",
    "train_dataset = None\n",
    "valid_dataset = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.zeros(len_all_data), all_data_y)):\n",
    "    print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "    if fold not in FOLD:\n",
    "        continue;\n",
    "    \n",
    "    all_data = pl.read_parquet(\"all_data.parquet\")\n",
    "\n",
    "    train = all_data[train_idx]\n",
    "    valid = all_data[val_idx][:10000]\n",
    "\n",
    "    del all_data\n",
    "    gc.collect()\n",
    "    \n",
    "    train_dataset = CustomDataset(train)\n",
    "    valid_dataset = CustomDataset(valid)\n",
    "    \n",
    "    del train, valid\n",
    "\n",
    "    modelmodule = LBModelModule(model_name, batch_size, train_dataset, valid_dataset)\n",
    "\n",
    "    EXP_NAME = f'5fold_chemberta_model6_fold{fold + 1}'\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filename=f\"model_{model_name}_fold{fold + 1}_{{val_map:.4f}}\",\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_map\",\n",
    "        mode=\"max\",\n",
    "        dirpath=f\"chemberta_v6_5fold/fold{fold+1}\",\n",
    "        save_top_k=5,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_map\", mode=\"max\", patience=5)\n",
    "  \n",
    "    progress_bar_callback = TQDMProgressBar(refresh_rate=1)\n",
    "   \n",
    "    callbacks = [checkpoint_callback, early_stop_callback, progress_bar_callback]\n",
    "  \n",
    "    trainer = L.Trainer(callbacks=callbacks, **trainer_params)\n",
    "\n",
    "    trainer.fit(modelmodule) #, train_dataloader, valid_dataloader\n",
    "    del train_dataset, valid_dataset # , train_dataloader, valid_dataloader\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb20aa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fe0f47-d37b-48f6-a0d3-bf822987fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = np.array(df['input_ids'])\n",
    "        self.attention_masks = np.array(df['attention_mask'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attention_masks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index],dtype=torch.int32),\n",
    "            \"attention_mask\": torch.tensor(self.attention_masks[index],dtype=torch.bool),\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f7ef5-2fd6-45aa-8d29-898e717e3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(callbacks=callbacks, **trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = [1,2,3,4,5]\n",
    "# !mv /kaggle/input/leash-bio-model-weights/model_ChemBERTa-77M-MTR_fold1_epoch3.ckpt /kaggle/input/leash-bio-model-weights/chemberta_v3_5fold/fold1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/\"+model_name)\n",
    "test_tokenize = pl.read_parquet(Path(data_dir, f'test_tokenized_77M_replace_dy.parquet'))\n",
    "test_dataset = CustomTestDataset(test_tokenize)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,pin_memory=True,num_workers=1,\n",
    "                              collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "fold_predictions = []\n",
    "\n",
    "for EPOCH in EPOCHS:\n",
    "    fold_predictions = []\n",
    "    for FOLD in range(1,6):\n",
    "    #     model_path = Path(f'/kaggle/input/leash-bio-model-weights/chemberta_v3_5fold/fold{FOLD}/model_ChemBERTa-77M-MTR_fold{FOLD}_epoch{EPOCH}.ckpt')\n",
    "        model_path = f'model_ChemBERTa-77M-MTR_fold{FOLD}_epoch{EPOCH}.ckpt'\n",
    "        # /home/sato/kag/chemberta_output/model2/model_ChemBERTa-77M-MTR_val_map=0.4487.ckpt\n",
    "        print(model_path)\n",
    "        modelmodule = LBModelModule.load_from_checkpoint(\n",
    "            checkpoint_path=model_path,\n",
    "            model_name=model_name,\n",
    "        )\n",
    "\n",
    "        predictions = trainer.predict(modelmodule, test_dataloader)\n",
    "\n",
    "        predictions = torch.cat(predictions).numpy()\n",
    "        fold_predictions.append(predictions)\n",
    "    avg_predictions = sum(fold_predictions) / len(fold_predictions)\n",
    "\n",
    "\n",
    "    pred_dfs = []\n",
    "    for i, protein_name in enumerate(PROTEIN_NAMES):\n",
    "        pred_dfs.append(\n",
    "            test_tokenize.with_columns(\n",
    "                pl.lit(protein_name).alias(\"protein_name\"),\n",
    "                pl.lit(avg_predictions[:, i]).alias(\"binds\"),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    pred_df = pl.concat(pred_dfs)\n",
    "\n",
    "    submit_df = (\n",
    "        pl.read_parquet(\"/kaggle/input/leash-BELKA/test.parquet\", columns=[\"id\", \"molecule_smiles\", \"protein_name\"])\n",
    "        .join(pred_df, on=[\"id\", \"protein_name\"], how=\"left\")\n",
    "        .select([\"id\", \"binds\"])\n",
    "        .sort(\"id\")\n",
    "    )\n",
    "    \n",
    "    submit_df.group_by('id').mean().write_csv(f\"chemberta_5fold_{EPOCH}.csv\")\n",
    "    print(submit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca41feb-278d-4cfe-9ae6-01bd0ddcee34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
